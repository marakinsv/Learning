{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q5QbSwGi32cr"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "import re\n",
        "import string # библиотека для работы со строками\n",
        "import nltk   # Natural Language Toolkit\n",
        "\n",
        "# загружаем библиотеку для лемматизации\n",
        "import pymorphy2 # Морфологический анализатор\n",
        "\n",
        "#from pymystem3 import Mystem\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "from sklearn.metrics import silhouette_samples\n",
        "from sklearn.metrics import roc_auc_score, mean_squared_error, \\\n",
        "    accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import validation_curve\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.datasets import make_classification\n",
        "#from xgboost.sklearn import XGBClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, GradientBoostingClassifier\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "#import xgboost as xgb\n",
        "#import lightgbm as lgb\n",
        "from catboost import CatBoostClassifier\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "#import umap\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
        "\n",
        "from sklearn.base import BaseEstimator\n",
        "from scipy.spatial.distance import cdist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qy7pRDsYJhXe"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "plt.rcParams[\"figure.figsize\"] = [10, 10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CkUsaO0B0oUE"
      },
      "outputs": [],
      "source": [
        "#!pip install --upgrade gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hXb-ljNDXbW0"
      },
      "outputs": [],
      "source": [
        "#!pip install umap-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nBqLAwOP3H7j"
      },
      "outputs": [],
      "source": [
        "#!pip install catboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zHkj3loeapDx"
      },
      "outputs": [],
      "source": [
        "#!pip install pymorphy2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lv9KAjG_fe9Y"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iCRu8iLH5Bao"
      },
      "outputs": [],
      "source": [
        "person_vectors = (\"brown\", \"black\", \"red\", \"orange\", \"yellow\", \"green\", \"blue\", \"purple\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zPw5zWXB7D4E"
      },
      "outputs": [],
      "source": [
        "df_vectors = pd.DataFrame([], columns=[\"description\", \"vectorId\"])\n",
        "\n",
        "for person_vector in person_vectors:\n",
        "    with open(\"drive/MyDrive/vectors/\" + person_vector + \".txt\", encoding=\"utf8\") as rf:\n",
        "        texts = rf.read()\n",
        "        texts = texts.replace('?', '.')\n",
        "        texts = texts.replace('!', '.')\n",
        "        texts = texts.replace('\\n', '')\n",
        "        for txt in texts.split(\".\"):\n",
        "            if len(txt) <= 1: continue\n",
        "            df_vectors = df_vectors.append({\"description\": txt.strip(), \"vectorId\": person_vectors.index(person_vector)}, ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "piwpVCATILSB"
      },
      "outputs": [],
      "source": [
        "df_vectors.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JfiXx77xAiX7"
      },
      "outputs": [],
      "source": [
        "df_vectors.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hVZY2W0PYUDH"
      },
      "outputs": [],
      "source": [
        "# загружаем список стоп-слов для русского\n",
        "nltk.download('stopwords')\n",
        "stop_words = nltk.corpus.stopwords.words('russian')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kj3QI_-RcP1u"
      },
      "outputs": [],
      "source": [
        "word_tokenizer = nltk.WordPunctTokenizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JBN4iG9FnQMM"
      },
      "outputs": [],
      "source": [
        "# инициализируем лемматизатор\n",
        "morph = pymorphy2.MorphAnalyzer()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_cONN7iaUSn"
      },
      "source": [
        "**Предобработка данных.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uQ_1UukLJhXh"
      },
      "outputs": [],
      "source": [
        "def words_only(text):\n",
        "    regex = re.compile(\"[А-Яа-яA]+\")\n",
        "    try:\n",
        "        return \" \".join(regex.findall(text))\n",
        "    except:\n",
        "        return \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NZ6RkzZXb437"
      },
      "outputs": [],
      "source": [
        "#df_vectors[\"description\"] = df_vectors[\"description\"].apply(lambda txt: words_only(txt))\n",
        "\n",
        "# Токенизация\n",
        "df_vectors[\"description\"] = df_vectors[\"description\"].apply(lambda txt: word_tokenizer.tokenize(txt))\n",
        "\n",
        "# Удаление стоп-слов\n",
        "df_vectors[\"description\"] = df_vectors[\"description\"].apply(\n",
        "    lambda tokens: [word.lower() for word in tokens if (word not in string.punctuation and word not in stop_words and word.isalpha())])\n",
        "\n",
        "# Лемматизация\n",
        "df_vectors[\"description\"] = df_vectors[\"description\"].apply(lambda txt: [morph.parse(word)[0].normal_form for word in txt])\n",
        "\n",
        "#mystem = Mystem()\n",
        "\n",
        "#df_vectors[\"description\"] = df_vectors[\"description\"].apply(lambda txt: ' '.join([mystem.lemmatize(word) for word in txt]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sRIlvXczxabg"
      },
      "outputs": [],
      "source": [
        "df_vectors.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWPd5TSfK0l4"
      },
      "source": [
        "**========================= WORTOVEC =============================**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dMSm7DbIJhXi"
      },
      "outputs": [],
      "source": [
        "#model = Word2Vec(df_vectors[\"description\"].values, vector_size=300, window=5, min_count=5, workers=4)\n",
        "\n",
        "model = KeyedVectors.load_word2vec_format('drive/MyDrive/186/model.bin', binary=True) # tayga-func_upos_skipgram_300_5_2019"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0QLRRzIAijd"
      },
      "source": [
        "**Средний вектор с весами tf-idf**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0BpBhyCDAh9y"
      },
      "outputs": [],
      "source": [
        "class TfidfEmbeddingVectorizer(object):\n",
        "    def __init__(self, word2vec):\n",
        "        self.word2vec = word2vec\n",
        "        self.word2weight = None\n",
        "        self.dim = len(w2v.popitem()[1])\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        tfidf = TfidfVectorizer(analyzer=lambda x: x)\n",
        "        tfidf.fit(X)\n",
        "        max_idf = max(tfidf.idf_)\n",
        "        self.word2weight = defaultdict(\n",
        "            lambda: max_idf,\n",
        "            [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()])\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        return np.array([\n",
        "                np.mean([self.word2vec[w] * self.word2weight[w]\n",
        "                         for w in words if w in self.word2vec] or\n",
        "                        [np.zeros(self.dim)], axis=0)\n",
        "                for words in X\n",
        "            ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OdkQr1xUJhXi"
      },
      "outputs": [],
      "source": [
        "words = []\n",
        "\n",
        "for w in model.index_to_key:\n",
        "    idx = w.index('_')\n",
        "    words.append(w[:idx])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8xDXGxn7A2Hc"
      },
      "outputs": [],
      "source": [
        "#w2v = dict(zip(model.wv.index_to_key, model.wv.vectors))\n",
        "w2v = dict(zip(words, model.vectors))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EUY322rEAtsY"
      },
      "outputs": [],
      "source": [
        "tfidfEmbVect = TfidfEmbeddingVectorizer(w2v)\n",
        "\n",
        "embedding_train = tfidfEmbVect.fit(df_vectors[\"description\"].values).transform(df_vectors[\"description\"].values)\n",
        "\n",
        "y_train = df_vectors[\"vectorId\"].values.copy().astype(np.int8)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Находим аномальные значения."
      ],
      "metadata": {
        "id": "231QTjG2NfeI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oxs3560UJhXj"
      },
      "outputs": [],
      "source": [
        "dbs = DBSCAN(eps=0.5, min_samples=2, metric=\"euclidean\")\n",
        "\n",
        "y_dbs = dbs.fit_predict(embedding_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jKZt9ughJhXj"
      },
      "outputs": [],
      "source": [
        "idx = np.where(y_dbs == -1)\n",
        "np.unique(y_train[np.where(y_dbs >= 0)], return_counts=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "В описании желтого вектора большее количество аномальных слов."
      ],
      "metadata": {
        "id": "U48Q6B3AOZRA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KGKIDw6QJhXj"
      },
      "outputs": [],
      "source": [
        "print(embedding_train[idx].shape)\n",
        "embedding_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IP3bpwXzJhXj"
      },
      "outputs": [],
      "source": [
        "# Отбрасываем аномальные значения\n",
        "embedding_train = embedding_train[idx]\n",
        "y_train = y_train[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dw_iQHpIJhXj"
      },
      "outputs": [],
      "source": [
        "#p.save(\"OTUS/project/embedding.npy\", embedding_train)\n",
        "#np.save(\"OTUS/project/vectorIds.npy\", y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qvlOgDTkJhXj"
      },
      "outputs": [],
      "source": [
        "tsne = TSNE(n_components=2, learning_rate='auto', init='random')\n",
        "embedding_tsne = tsne.fit_transform(embedding_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4lvZ8b0vJhXk"
      },
      "outputs": [],
      "source": [
        "plt.scatter(embedding_tsne[:, 0], embedding_tsne[:, 1], c=y_train, cmap='viridis', label=y_train)\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Все вектора располагаются в одном кластере."
      ],
      "metadata": {
        "id": "5d4Ehme8Ml3y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xHnM2Ba1JhXk"
      },
      "outputs": [],
      "source": [
        "idxs = np.where((y_train == 0) | (y_train == 1))[0]\n",
        "\n",
        "plt.scatter(embedding_tsne[idxs, 0], embedding_tsne[idxs, 1], c=y_train[idxs], cmap='viridis', label=[0, 1])\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Видно, что два вектора разделимы."
      ],
      "metadata": {
        "id": "SR8wfm1RMw-d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gmo1IrSA2os_"
      },
      "outputs": [],
      "source": [
        "#classifier = RandomForestClassifier(n_estimators=200)\n",
        "classifier = CatBoostClassifier(loss_function=\"MultiClass\", eval_metric=\"AUC\", custom_metric=\"F1\", \n",
        "                           random_seed=42, logging_level=\"Silent\", use_best_model=False)\n",
        "\n",
        "classifier.fit(embedding_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eI4aTXCGJhXk"
      },
      "outputs": [],
      "source": [
        "classifier.best_score_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ja59hCWVJhXk"
      },
      "outputs": [],
      "source": [
        "data = pd.DataFrame([], columns=[\"description\"])\n",
        "\n",
        "with open(\"drive/MyDrive/test/darvin.txt\", encoding=\"utf8\") as rf:\n",
        "    texts = rf.read()\n",
        "    texts = texts.replace('?', '.')\n",
        "    texts = texts.replace('!', '.')\n",
        "    texts = texts.replace('\\n', '')\n",
        "    for txt in texts.split(\".\"):\n",
        "        if len(txt) <= 1: continue\n",
        "        data = data.append({\"description\": txt.strip()}, ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JP0DAe0fJhXk"
      },
      "outputs": [],
      "source": [
        "# Токенизация\n",
        "data[\"description\"] = data[\"description\"].apply(lambda txt: word_tokenizer.tokenize(txt))\n",
        "\n",
        "# Удаление стоп-слов\n",
        "data[\"description\"] = data[\"description\"].apply(\n",
        "    lambda tokens: [word.lower() for word in tokens if (word not in string.punctuation and word not in stop_words and word.isalpha())])\n",
        "\n",
        "# Лемматизация\n",
        "data[\"description\"] = data[\"description\"].apply(lambda txt: [morph.parse(word)[0].normal_form for word in txt])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aGOc9rPqJhXk"
      },
      "outputs": [],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EjN94DLPJhXk"
      },
      "outputs": [],
      "source": [
        "embedding_test = tfidfEmbVect.transform(data[\"description\"].values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ClsU0kyhJhXk"
      },
      "outputs": [],
      "source": [
        "#np.save(\"OTUS/project/embedding_test.npy\", embedding_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nsh6KTWTJhXl"
      },
      "outputs": [],
      "source": [
        "embedding = np.concatenate((embedding_train, embedding_test), axis=0)\n",
        "labels = np.concatenate((y_train, np.array([8] * embedding_test.shape[0])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O7FhNmVnJhXl"
      },
      "outputs": [],
      "source": [
        "tsne = TSNE(n_components=2, learning_rate='auto', init='random')\n",
        "embedding_tsne = tsne.fit_transform(embedding)\n",
        "\n",
        "plt.scatter(embedding_tsne[:, 0], embedding_tsne[:, 1], c=labels, cmap='viridis', label=labels)\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2mR8EpdIJhXl"
      },
      "outputs": [],
      "source": [
        "# Отбрасываем значения тестовых данных, которые не попадают ни в один из векторных кластеров\n",
        "idxs = []\n",
        "idx = 0\n",
        "for values in embedding_test:\n",
        "    for vectorId in range(len(person_vectors)):\n",
        "        vectorIds = np.where(y_train == vectorId)[0]\n",
        "        #arr_max = np.mean(embedding_train[vectorIds], axis=0) + 3 * np.std(embedding_train[vectorIds], axis=0)\n",
        "        #arr_min = np.mean(embedding_train[vectorIds], axis=0) - 3 * np.std(embedding_train[vectorIds], axis=0)\n",
        "        arr_max = np.max(embedding_train[vectorIds], axis=0)\n",
        "        arr_min = np.min(embedding_train[vectorIds], axis=0)\n",
        "    \n",
        "        if np.all(values <= arr_max) and np.all(values >= arr_min): # Точка в кластере описания какого-либо вектора\n",
        "            idxs.append(idx)\n",
        "            break\n",
        "    idx += 1\n",
        "\n",
        "print(embedding_test.shape[0], len(idxs))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Отбросили 117 значений как аномальные, не входящие в кластер векторов человека."
      ],
      "metadata": {
        "id": "3m0YBcouPReN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lBoka1LiJhXl"
      },
      "outputs": [],
      "source": [
        "embedding = np.concatenate((embedding_train, embedding_test[idxs]), axis=0)\n",
        "labels = np.concatenate((y_train, np.array([8] * embedding_test[idxs].shape[0])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eMJe96CWJhXl"
      },
      "outputs": [],
      "source": [
        "tsne = TSNE(n_components=2, learning_rate='auto', init='random')\n",
        "embedding_tsne = tsne.fit_transform(embedding)\n",
        "\n",
        "plt.scatter(embedding_tsne[:, 0], embedding_tsne[:, 1], c=labels, cmap='viridis', label=labels)\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Желтым цветом обозначены тестовые данные, видно, что они в кластере векторов человека."
      ],
      "metadata": {
        "id": "WCpAlw05PicL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v3WeGoFvJhXl"
      },
      "outputs": [],
      "source": [
        "#np.save(\"OTUS/project/embedding_test.npy\", embedding_test[idxs])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aL1uXanNJhXl"
      },
      "outputs": [],
      "source": [
        "predict_proba = classifier.predict_proba(embedding_test[idxs])\n",
        "#predict_proba"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SvboFszuJhXl"
      },
      "outputs": [],
      "source": [
        "for idx in range(len(person_vectors)):\n",
        "    print(np.mean(predict_proba[:, idx]), \"-\", person_vectors[idx])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Коричневый вектор преобладает, о чем и говорил В.К. Толкачев.\n",
        "Оранжевый - дисциплина\n",
        "Черный - любит физическую работу."
      ],
      "metadata": {
        "id": "RblXZXovP8Jo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aLRrH-2FJhXl"
      },
      "outputs": [],
      "source": [
        "for idx in range(len(person_vectors)):\n",
        "    print(np.max(predict_proba[:, idx]), \"-\", person_vectors[idx])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AyYtKQMLJhXl"
      },
      "outputs": [],
      "source": [
        "# Посмотрим какими качествами будет обладать человек, если у него преобладают три вектора \n",
        "# (коричневый, черный, красный)\n",
        "# Так ли это на самом деле, пока вопрос.\n",
        "idx1 = words.index(\"аккуратность\")     # brown vector\n",
        "idx2 = words.index(\"скромность\")       # black vector\n",
        "idx3 = words.index(\"решительность\")    # red vector\n",
        "\n",
        "vec = model[model.index_to_key[idx1]] + model[model.index_to_key[idx2]] + model[model.index_to_key[idx3]]\n",
        "\n",
        "model.similar_by_vector(vec)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LMIXzb3JJhXl"
      },
      "outputs": [],
      "source": [
        "# Посмотрим какими качествами будет обладать человек, если у него преобладают три вектора \n",
        "# (коричневый, черный, красный) и красный не принят (невроз).\n",
        "idx1 = words.index(\"аккуратность\")     # brown vector\n",
        "idx2 = words.index(\"скромность\")       # black vector\n",
        "idx3 = words.index(\"трусливость\")      # red vector\n",
        "\n",
        "vec = model[model.index_to_key[idx1]] + model[model.index_to_key[idx2]] + model[model.index_to_key[idx3]]\n",
        "\n",
        "model.similar_by_vector(vec)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hT1afGR5JhXm"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "PROJECT.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}